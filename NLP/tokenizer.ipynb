{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21268699-13d2-4532-9c3a-b8868c566f25",
   "metadata": {},
   "source": [
    "Basic terminology\n",
    "#Corpus:-paragraph is usually called as Corpus\n",
    "#documents:-Sentences\n",
    "#Vocabulary:-Unique words\n",
    "Tokenization\n",
    "\n",
    "Sure, here are the key differences between NLTK and spaCy in bullet points:\r\n",
    "\r\n",
    "### NLTK (Natural Language Toolkit)\r\n",
    "- **Purpose:** Designed for research and education in NLP.\r\n",
    "- **Performance:** Generally slower, adequate for research but not optimized for large-scale production.\r\n",
    "- **Usability:** Steeper learning curve, extensive functions, flexible and versatile.\r\n",
    "- **Features:** Wide array of traditional NLP tools, many corpora and lexical resources, suitable for detailed linguistic analysis.\r\n",
    "- **Extensibility:** Highly extensible with numerous add-ons and community modules.\r\n",
    "- **Integration:** Provides tools for creating custom NLP pipelines, less focused on integration with modern machine learning frameworks.\r\n",
    "- **Community:** Large academic following, extensive support in research contexts.\r\n",
    "\r\n",
    "### spaCy\r\n",
    "- **Purpose:** Designed for practical, real-world usage, especially in production environments.\r\n",
    "- **Performance:** Highly optimized for speed and efficiency, suitable for large-scale and real-time applications.\r\n",
    "- **Usability:** Easy to use with a straightforward API, excellent documentation and tutorials.\r\n",
    "- **Features:** State-of-the-art pre-trained models, efficient tokenization, named entity recognition, part-of-speech tagging, dependency parsing, integrates well with deep learning libraries.\r\n",
    "- **Extensibility:** Supports custom pipelines and extensions, focused on robust default models and components.\r\n",
    "- **Integration:** Designed to integrate easily with NLP and machine learning frameworks, supports word vectors and deep learning pipelines.\r\n",
    "- **Community:** Rapidly growing community, strong focus on industry applications, active development from Explosion AI.\r\n",
    "\r\n",
    "Choosing between the two depends on your specific needs:\r\n",
    "- **NLTK:** Best for researchers and educators needing a broad toolkit.\r\n",
    "- **spaCy:** Ideal for developers and data scientists needing efficient, production-ready NLP solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c531c001-f864-4038-8e09-cbca7901b67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "707f752b-b907-42d4-8875-ca1b697f89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome to My notebook.\n",
    "Please, Dont't Give up bruhh,You can do this.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fb82af0-2e8e-49a7-9748-4f86c265b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome to My notebook.\n",
      "Please, Dont't Give up bruhh,You can do this.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0868636-3987-4b01-927a-8782463a0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "#prargraph ----> sentences\n",
    "from nltk import sent_tokenize  #sentence tokkenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3b1d81a-711b-4475-a60f-ed3afbd87b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85965926-b6c8-40b7-b3bc-fd882037b975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee50185e-95b8-4fc9-a322-047465071ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokkenizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0520ad4-cc33-4815-8cd7-7caa84e5f701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'My',\n",
       " 'notebook',\n",
       " '.',\n",
       " 'Please',\n",
       " ',',\n",
       " \"Dont't\",\n",
       " 'Give',\n",
       " 'up',\n",
       " 'bruhh',\n",
       " ',',\n",
       " 'You',\n",
       " 'can',\n",
       " 'do',\n",
       " 'this',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)  #para----> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1827243e-f1ce-4dfc-bd1a-6d02c79552cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', 'to', 'My', 'notebook', '.']\n",
      "['Please', ',', \"Dont't\", 'Give', 'up', 'bruhh', ',', 'You', 'can', 'do', 'this', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:  #sentence -------> words\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0805871-3c0e-4072-8106-9d24ae925c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more tokenize techniques are \n",
    "#1.wordpunt -> in this as Nikhil's is not tokenize in word tokenize but it will tokenize 's too.\n",
    "#2 treebankWordtokenizer-> in this it will trat \".(fullstop)\"as part of words and not tokenize it ,but it will tokenize the last fullstop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3a0633-3478-4488-afe6-eee820a76ac2",
   "metadata": {},
   "source": [
    "STEMMING\n",
    "Definition: Stemming is the process of reducing words to their root or base form.\r\n",
    "Purpose: To normalize words so that different forms of the same word are treated as the same.\r\n",
    "2. Benefits of Stemming\r\n",
    "Reduces Data Size: By converting words to their stems, you reduce the number of unique words.\r\n",
    "Improves Search and Matching: Helps in finding documents by matching different forms of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc8109-0ff2-43ff-8e2e-a094e722f1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
